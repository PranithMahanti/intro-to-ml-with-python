# Chapter 2 - Supervised Learning
- Any intuition derived from datasets with few features (also called
low-dimensional datasets) might not hold in datasets with many features (high-
dimensional datasets)

### R^2 Score
We can also evaluate the model using the score method, which for regressors returns
the R2 score. The R2 score, also known as the coefficient of determination, is a meas‐
ure of goodness of a prediction for a regression model, and yields a score between 0
and 1. A value of 1 corresponds to a perfect prediction, and a value of 0 corresponds
to a constant model that just predicts the mean of the training set responses, y_train:

## KNN
### Advantages & Disadvantages of KNN prediction models
- One of the strengths of k-NN is that the model is very easy to understand, and often
gives reasonable performance without a lot of adjustments. 
- Using this algorithm is a good baseline method to try before considering more advanced techniques. 
- Building the nearest neighbors model is usually very fast, but when your training set is very
large (either in number of features or in number of samples) prediction can be slow.
- When using the k-NN algorithm, it’s important to preprocess your data.
- This approach often does not perform well on datasets with many features
(hundreds or more), and it does particularly badly with datasets where most features
are 0 most of the time (so-called sparse datasets).

## Linear Models
- Linear models make a prediction using a linear function of the input features.

### Linear Models for Regression
- For regression, the general prediction formula for a linear model looks as follows:
ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b
- Linear models for regression can be characterized as regression models for which the
prediction is a line for a single feature, a plane when using two features, or a hyper‐
plane in higher dimensions 

### Linear regression
- A.K.A. Ordinary Least Squares (OLS)
- Linear regression finds the parameters w and b that mini‐
mize the mean squared error between predictions and the true regression targets, y,
on the training set.
- The mean squared error is the sum of the squared differences
between the predictions and the true values.
- Linear regression has no parameters,
which is a benefit, but it also has no way to control model complexity.

- The “slope” parameters (w), also called weights or coefficients, are stored in the coef_
attribute, while the offset or intercept (b) is stored in the intercept_ attribute
	- intercept_ : Always a single floating point number.
	- coef_ : A Numpy array with an entry for each feature


